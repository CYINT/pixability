{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit (windows store)"
  },
  "interpreter": {
   "hash": "24ca19356ca694c297a387d280c5e598843f5f2f587af9a61e7a5339248c8f6a"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Pixability Data Science Technical Assessment\n",
    "\n",
    "The goal of this exerise is to create a *basic* machine learning model that uses the criteo dataset to predict the optimal *bid price* for the next period based on the features.\n",
    "\n",
    "The criteo dataset is located in the `data.tsv` file. \n",
    "\n",
    "Instructions for the assignment, including a data dictionary for the Criteo dataset, can be found in the accompanying `Pixability_Data_Science_Assesment.pdf` file.\n",
    "\n",
    "# Findings and Conclusions\n",
    "\n",
    "An Ordinary Least Squares Linear Regression Model is a good optimal bid predictor for this data set, but not the best predictor. The model does not satisfy the Gauss-Markov Conditions for efficiency in a linear estimator. dodel adjusted $ R^2 $ was only 0.104 but $ RMSE $ was very good, only off on average by a fifth of a cent. The model errors will be very sensitive to inputs outside of a normal standard deviation.\n",
    "\n",
    "    Train rmse: 0.0020056291132341726    \n",
    "    Test rmse: 0.0020011741446683117   \n",
    "    \n",
    "\n",
    "# Methodology\n",
    "\n",
    "The experiment was conducted as follows:\n",
    "\n",
    "1. Load the dataset into memory\n",
    "2. Explore the dataset\n",
    "3. Clean and tidy the dataset\n",
    "4. Select features to train the model.\n",
    "5. Train the model and test for gauss markov conditions.  \n",
    "6. Calculate model metrics ($ R^2, RMSE $)  \n",
    "\n",
    "# Recommended Follow Up\n",
    "\n",
    "1. Explore other non-linear models as they may provide better results. Models such as Random Forests, Decision-Trees, KNearestNeighbors, or a Deep Learning model will likely perform well.\n",
    "2. Perform additional grid-searching or dimensionality reduction around feature selection.\n",
    "3. Explore transforming the target variable to potentially address homosckedasticity issues. \n",
    "4. Build a model to forecast the seasonal/trend component of the cost to be used in an ensemble in making predictions.\n",
    "\n",
    "## 1. Loading the DataSet\n",
    "\n",
    "Because this is large data, we will only pull a sample of 1000 rows to explore some of the data values and make decisions as to how to clean/tidy the data.\n",
    "\n",
    "First we install some packages.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install prince\n",
    "!pip install sklearn\n",
    "!pip install seaborn\n",
    "!pip install matplotlib\n",
    "!pip install statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_sample = pd.read_csv('data.tsv', sep='\\t', nrows=1000)\n",
    "data_sample.info()"
   ]
  },
  {
   "source": [
    "## 2. Explore the Data\n",
    "\n",
    "We can explore the sample data to get a look at some of the values, and some of the ranges of values."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "data_sample.head()"
   ]
  },
  {
   "source": [
    "Lets look at some of the possible value ranges:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sample.describe()"
   ]
  },
  {
   "source": [
    "## 3. Clean and Tidy the DataSet\n",
    "\n",
    "Since the data is timeseries and sorted by timestamp, we cannot run any exploratory plots on the data just yet. Instead, let's import the data into chunks and process it so that we can eliminate columns that are not necessary and try to fit the full data in memory.  \n",
    "\n",
    "It looks like `uid, conversion_timestamp, conversion_id, attribution` are not useful features to include because they are metadata about the user/conversion that are either known after the fact or should not impact the final decision.\n",
    "\n",
    "We also need to change the different cat columns into categoricals/dummy variables so that they are not interpreted as numericals.\n",
    "\n",
    "We assume that the cost for the ad is handled like most ad bidding systems where the advertiser only pays one penny more than the 2nd highest bidder, which means that all `cost` values for impressions resulting in conversions are \"optimal\". We will not consider `cpo` for this model as we do not have any information on order profitability so it cannot be used to impact our decision-making in terms of an optimal bid.\n",
    "\n",
    "Now we will import the data by chunks, excluding those columns, dropping any record that did not result in a conversion and did not result in a positive cost and performing the necessary cleaning/tidying steps.\n",
    "\n",
    "The file size is ~2GB, so it should roughly fit into available memory, especially after preprocessing."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    data.drop(['uid', 'conversion_timestamp', 'conversion_id', 'attribution', 'cpo'], axis='columns', inplace=True)\n",
    "    data = data.loc[data['conversion'] == 1]\n",
    "    return data\n",
    "\n",
    "data_list = []\n",
    "\n",
    "for chunk in pd.read_csv('data.tsv', sep='\\t', chunksize=10000):\n",
    "    preprocessed_chunk = preprocess(chunk)\n",
    "    data_list.append(preprocessed_chunk)\n",
    "\n",
    "data_tidied = pd.concat(data_list)\n",
    "data_tidied[['campaign', 'cat1','cat2','cat3','cat4','cat5','cat6','cat7','cat8','cat9']] = data_tidied[['campaign', 'cat1','cat2','cat3','cat4','cat5','cat6','cat7','cat8','cat9']].astype('category')\n",
    "data_tidied.index = data_tidied['timestamp']\n",
    "data_tidied.drop(['conversion','timestamp','click'], axis='columns', inplace=True) # Conversion / Click always 1\n",
    "data_tidied.info()"
   ]
  },
  {
   "source": [
    "We were able to reduce the size of the dataset to 41.5 MB after initial preprocessing.\n",
    "\n",
    "Now let's seperate the target variable from the dataset, and move on to do some feature selection."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = data_tidied['cost']\n",
    "X = data_tidied.drop('cost', axis='columns')\n",
    "print(Y.head())\n",
    "print(X.head())"
   ]
  },
  {
   "source": [
    "## 4. Feature Selection\n",
    "\n",
    "First let's separate the categorical features from the numeric features so we can work with them individually.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_categorical = X.loc[:, ['campaign', 'cat1', 'cat2', 'cat3', 'cat4', 'cat5', 'cat6', 'cat7', 'cat8', 'cat9']]\n",
    "X_numeric = X.loc[:, ['click_pos', 'click_nb', 'time_since_last_click']]\n",
    "\n",
    "print(X_categorical.info())\n",
    "print(X_numeric.info())"
   ]
  },
  {
   "source": [
    "Let's start by analyzing for multicolinearity in the features."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.heatmap(X_numeric.corr())"
   ]
  },
  {
   "source": [
    "This chart suggests that there is strong multicolinearity between click_pos and click_nb. One of them will have to be dropped."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Y.corr(X_numeric['click_pos']))\n",
    "Y.corr(X_numeric['click_nb'])\n"
   ]
  },
  {
   "source": [
    "There is a slightly higher correlation between `click_nb` and the target than `click_pos` so let's drop `click_pos`. Other methods of deciding which variable to drop include dropping the column with the higher VIF, or training models and comparing $ R^2 $ values"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_numeric_decolineated = X_numeric.drop(['click_pos'], axis='columns')\n",
    "sns.heatmap(X_numeric_decolineated.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_numeric_decolineated.info()"
   ]
  },
  {
   "source": [
    "Because we have so many categorical values, let's use frequency encoding to reduce the dimensionality. Frequency encoding will weight each of the categories according to how often they show up in the dataset. This allows us to avoid the curse of dimensionality while possibly preserving some information about the recurrence of certain features associated with conversions."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency_encode(column, df):\n",
    "    category = (X_categorical.groupby(column).size()) / len(X_categorical)\n",
    "    df[f'{column}_encode'] = df[column].apply(lambda x: category.loc[x])\n",
    "    return df\n",
    "\n",
    "frequency_encode('cat1', X_categorical)\n",
    "frequency_encode('cat2', X_categorical)\n",
    "frequency_encode('cat3', X_categorical)\n",
    "frequency_encode('cat4', X_categorical)\n",
    "frequency_encode('cat5', X_categorical)\n",
    "frequency_encode('cat6', X_categorical)\n",
    "frequency_encode('cat7', X_categorical)\n",
    "frequency_encode('cat8', X_categorical)\n",
    "frequency_encode('cat9', X_categorical)\n",
    "frequency_encode('campaign', X_categorical)\n",
    "\n",
    "X_categorical_encoded = X_categorical.drop(['campaign', 'cat1', 'cat2', 'cat3', 'cat4', 'cat5', 'cat6', 'cat7', 'cat8', 'cat9'], axis='columns')\n",
    "X_categorical_encoded.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(X_categorical_encoded.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Y.corr(X_categorical_encoded['cat3_encode']))\n",
    "print(Y.corr(X_categorical_encoded['campaign_encode']))"
   ]
  },
  {
   "source": [
    "`cat3` has higher correlation with the target variable, and high multicolinearity with `campaign`, so we will drop `campaign`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_categorical_decolineated = X_categorical_encoded.drop('campaign_encode', axis='columns')\n",
    "sns.heatmap(X_categorical_decolineated.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_full = pd.concat([X_numeric_decolineated, X_categorical_decolineated], axis='columns')\n",
    "sns.heatmap(X_full.corr())"
   ]
  },
  {
   "source": [
    "Let's select the top five features. We selected a k of ten arbitrarily here for this exercise. k can be optimized by comparing the results of models iteratively for as many k as there are feature columns, and counting down from columns-1, ... 1."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "\n",
    "selection = SelectKBest(f_regression, k=10).fit(X_full, Y)\n",
    "features = X_full.columns[selection.get_support()].tolist()\n",
    "print(features)\n",
    "X_selected = X_full.loc[:, features]\n",
    "X_selected.head()"
   ]
  },
  {
   "source": [
    "## 5. Train the model and forecast\n",
    "\n",
    "This step involves -   \n",
    "1. Detecting and correcting for seasonality/trend.  \n",
    "2. Splitting the data into a train/test set.  \n",
    "3. Train the model and calculate errors.  \n",
    "4. Determine if the model satisfies the Gauss Markov conditions.  \n",
    "\n",
    "\n",
    "### Fast Fourier Transform\n",
    "\n",
    "We detect the different periods of seasonality using a fast fourier transform (FFT). FFT works by decomposing the signal into it's component frequencies. This allows us to identify strong recurring frequencies to determine a period for seasonality."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats, special, fft, signal as sig\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "import numpy as np\n",
    "\n",
    "fft_results = fft.fft(np.array(Y))\n",
    "power = np.abs(fft_results)\n",
    "freq = fft.fftfreq(len(Y))\n",
    "mask = freq >= 0\n",
    "freq = freq[mask]\n",
    "power = power[mask]\n",
    "\n",
    "peaks = sig.find_peaks(power[freq >= 0])[0]\n",
    "peak_freq = freq[peaks]\n",
    "peak_power = power[peaks]\n",
    "period = 1/peak_freq\n",
    "fft_output = pd.DataFrame()\n",
    "\n",
    "fft_output['index'] = peaks\n",
    "fft_output['freq'] = peak_freq\n",
    "fft_output['amplitude'] = peak_power\n",
    "fft_output['period'] = period\n",
    "fft_output['period_rounded'] = np.round(fft_output['period']).astype(int)\n",
    "fft_output['fft'] = fft_results[peaks]\n",
    "fft_output.sort_values('amplitude', ascending=False, inplace=True)\n",
    "\n",
    "fft_output.head()"
   ]
  },
  {
   "source": [
    "Fast Fourier transform detects a strong signal every 26006 periods within timestamp.\n",
    "\n",
    "We can now use the seasonal_decompose functions to remove the seasonality."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seasonal_output = seasonal_decompose(Y, period=int(fft_output.iloc[0]['period_rounded']))\n",
    "seasonal_output.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trend = pd.DataFrame(seasonal_output.trend).fillna(0)\n",
    "detrended_signal = pd.DataFrame(Y - trend['trend'])\n",
    "detrended_signal.columns = ['detrended']\n",
    "detrended_seasonality = seasonal_decompose(detrended_signal, period=int(fft_output.iloc[0]['period_rounded']))\n",
    "decycled_signal = detrended_signal['detrended'] - detrended_seasonality.seasonal\n",
    "decycled_signal = pd.DataFrame(decycled_signal)\n",
    "decycled_signal.columns = ['decycled']\n",
    "decycled_signal.head()"
   ]
  },
  {
   "source": [
    "Let's test to see if the signal is thoroughly deseasonalized by leveraging the augmented dickey-fuller test (ADF). ADF tests for the presence of a unit root, which implies seasonality. The null hypothesis is that a unit root exists, and to reject the null means that the data is stationary and has been deseasonalized. We seek to validate stationarity at a 95% confidence level so we seek a p-value of 0.05 or less. We will use a subsample of the time series data to test for stationarity for performance reasons."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller as adf\n",
    "\n",
    "stat = adf(decycled_signal.loc[0:50000,'decycled'])\n",
    "print(f\"Statistic is {stat[0]}\")\n",
    "print(f\"P-value is {stat[1]}\")"
   ]
  },
  {
   "source": [
    "The p-value is less than our alpha of 0.05, thus we reject the null hypothesis and conclude the data is likely stationary.\n",
    "\n",
    "We can use the KPSS test to test for trend-stationarity in the same fashion, but we will skip that for this exercise.\n",
    "\n",
    "With our decycled signal, we no longer need to worry about time series in our data and we can obtain a train/test split of our X and Y values so that we can train a linear regression model.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(X_selected)\n",
    "a = 0.01 - decycled_signal.min()\n",
    "X_scaled = scaler.transform(X_selected)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, decycled_signal, train_size=0.8, random_state=42)"
   ]
  },
  {
   "source": [
    "We will use statsmodels to produce a Linear Regression model and evaluate metrics to determine how effective it is at making predictions."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import statsmodels.stats.api as sms\n",
    "\n",
    "#Don't forget to add the constant to the dataset with statsmodels package\n",
    "X_train_constant = sm.add_constant(X_train)\n",
    "X_test_constant = sm.add_constant(X_test)\n",
    "\n",
    "\n",
    "\n",
    "model = sm.OLS(y_train, X_train_constant).fit()\n",
    "model.summary()"
   ]
  },
  {
   "source": [
    "The Jarque-Bera test for normality rejects the null hypothesis that the residuals of the model are normally distributed at a 95% confidence level.\n",
    "\n",
    "Let's check the homoskedasticity of the errors:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = sms.het_breuschpagan(model.resid, model.model.exog)\n",
    "stat, pval, fscore, fpval = test\n",
    "print(f'Lagrange statistic: {stat}')\n",
    "print(f'P-value: {pval}')\n",
    "print(f'F-Score: {fscore}')\n",
    "print(f'F-Score P-Value: {fpval}')\n"
   ]
  },
  {
   "source": [
    "The Breusch-Pagan test rejects the null that the errors are homoskedastic at a 95% confidence level.\n",
    "\n",
    "Based on the above, we can conclude that our linear model does not satisfy the Gauss-Markov conditions and is therefore not the best estimator. It is recommended that other estimators be explored to determine if greater precision from predictions can be achieved."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 6. Calculate RMSE"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tools.eval_measures import rmse\n",
    "\n",
    "y_predicted_train = model.predict(X_train_constant)\n",
    "residuals_train = (y_train['decycled'] - y_predicted_train)\n",
    "residualavg_train = (residuals_train**2).sum()\n",
    "rmse_train = np.sqrt(residualavg_train / len(residuals_train))\n",
    "print(f'Train rmse: {rmse_train}')\n",
    "\n",
    "\n",
    "y_predicted_test = model.predict(X_test_constant)\n",
    "residuals_test = (y_test['decycled'] - y_predicted_test)\n",
    "residualavg_test = (residuals_test**2).sum()\n",
    "rmse_test = np.sqrt(residualavg_test / len(residuals_test))\n",
    "print(f'Test rmse: {rmse_test}')\n",
    "\n",
    "print(f\"Y Train Min: {y_train['decycled'].min()}\")\n",
    "print(f\"Y Train Max: {y_train['decycled'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}